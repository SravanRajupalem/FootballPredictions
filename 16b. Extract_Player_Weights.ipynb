{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "url = 'https://www.transfermarkt.com/premier-league/transfers/wettbewerb/GB1/plus/?saison_id=2018&s_w=&leihe=0&leihe=1&intern=0&intern=1'\n",
        "\n",
        "response = requests.get(url, headers={'User-Agent': 'Custom5'})\n",
        "print(response.status_code)\n",
        "financial_data = response.text\n",
        "soup = BeautifulSoup(financial_data, 'html.parser')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install\n",
        "# !pip install pyjsparser\n",
        "# !pip install js2xml\n",
        "\n",
        "# Base\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import plotly.express as ex\n",
        "\n",
        "# Web Scraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# !pip install pyjsparser\n",
        "# from pyjsparser import parse\n",
        "import pyjsparser\n",
        "\n",
        "# Time Sleep\n",
        "import sys\n",
        "import time\n",
        "from datetime import datetime\n",
        "from termcolor import colored\n",
        "\n",
        "# GC\n",
        "import gc\n",
        "\n",
        "# Itertools\n",
        "import itertools\n",
        "\n",
        "# Grafikten Data Çekmek için\n",
        "import re\n",
        "import js2xml\n",
        "from itertools import repeat    \n",
        "from pprint import pprint as pp\n",
        "\n",
        "# String Manipulation\n",
        "import re \n",
        "\n",
        "# Configurations\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Helpers\n",
        "# -----------------------------------------------------------------------------\n",
        "# To learn process time\n",
        "def get_time(cond):\n",
        "    if cond == \"start\":\n",
        "        p = \"This process started at \"\n",
        "    elif cond == \"end\":\n",
        "        p = \"The process completed at \"\n",
        "    print(\"\") \n",
        "    print(colored(p + str(datetime.now().strftime(\"%H:%M:%S\")), \"green\", \"on_white\", attrs=[\"bold\",'reverse', 'blink']))\n",
        "    \n",
        "    \n",
        "def Filter(string, substr): \n",
        "        return [str for str in string if\n",
        "                any(sub in str for sub in substr)] \n",
        "    \n",
        "def NOTFilter(string, substr): \n",
        "    return [str for str in string if\n",
        "            any(sub not in str for sub in substr)] \n",
        "    \n",
        "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "page = 'https://sofifa.com/?r=220013&set=true&showCol%5B0%5D=ae&showCol%5B1%5D=oa&showCol%5B2%5D=pt&showCol%5B3%5D=vl&showCol%5B4%5D=hi&showCol%5B5%5D=wi&showCol%5B6%5D=pf&showCol%5B7%5D=bo&showCol%5B8%5D=pi&players%3Ftype=all&lg%5B0%5D=13&lg%5B1%5D=16&lg%5B2%5D=19&lg%5B3%5D=31&lg%5B4%5D=53&offset=0'\n",
        "\n",
        "pageTree = requests.get(page+str(0), headers=headers)\n",
        "pageSoup = BeautifulSoup(pageTree.content, 'html.parser')\n",
        "\n",
        "urls_list = []\n",
        "\n",
        "for i in pageSoup.find_all('a', {'class':\"bp3-menu-item\"}):\n",
        "    if 'FIFA' in i.get_text():\n",
        "        urls_list.append(i['href'][4:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-4-711cda6a1ee2>, line 73)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-711cda6a1ee2>\"\u001b[0;36m, line \u001b[0;32m73\u001b[0m\n\u001b[0;31m    try:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "def extract_weights(year, num_pages=10):\n",
        "\n",
        "    number_of_pages = num_pages\n",
        "    page = \"https://sofifa.com/?r=\" + year + \"&set=true&showCol[]=ae&showCol[]=oa&showCol[]=pt&showCol[]=vl&showCol[]=hi&showCol[]=wi&showCol[]=pf&showCol[]=bo&showCol[]=pi&players?type=all&lg%5B%5D=13&lg%5B%5D=16&lg%5B%5D=19&lg%5B%5D=31&lg%5B%5D=53&offset=\"\n",
        "\n",
        "    Players_List = []\n",
        "    Age_List = []\n",
        "    OR_List = []\n",
        "    PR_List = []\n",
        "    Team_List = []\n",
        "    contract_List = []\n",
        "    height_List = []\n",
        "    weight_List = []\n",
        "    PF_List = []\n",
        "    Value_List = []\n",
        "    FullName_List = []\n",
        "    \n",
        "    for offset in range(0, number_of_pages):\n",
        "        pageTree = requests.get(page+str(offset * 60), headers=headers)\n",
        "        pageSoup = BeautifulSoup(pageTree.content, 'html.parser')\n",
        "        \n",
        "        Players = pageSoup.find_all(\"a\", {\"class\": \"tooltip\"})\n",
        "        Age = pageSoup.find_all(\"td\", {\"class\": \"col col-ae\"})\n",
        "        OR = pageSoup.find_all(\"td\", {\"class\": \"col col-oa col-sort\"})\n",
        "        PR = pageSoup.find_all(\"td\", {\"class\": \"col col-pt\"})\n",
        "        Team = pageSoup.select(\"a[href*=\\/team\\/]\")\n",
        "        contract = pageSoup.find_all(\"div\", {\"class\": \"sub\"})\n",
        "        height = pageSoup.find_all(\"td\", {\"class\": \"col col-hi\"})\n",
        "        weight = pageSoup.find_all(\"td\", {\"class\": \"col col-wi\"})\n",
        "        PF = pageSoup.find_all(\"td\", {\"class\": \"col col-pf\"})\n",
        "        Value = pageSoup.find_all(\"td\", {\"class\": \"col col-vl\"})\n",
        "        FullName = pageSoup.find_all(\"a\", {\"class\": \"tooltip\"})\n",
        "\n",
        "        for i in range(0,60):\n",
        "            try:\n",
        "                Players_List.append(Players[i].get_text().strip())\n",
        "            except:\n",
        "                pass   \n",
        "            try:\n",
        "                Age_List.append(Age[i].get_text().strip())\n",
        "            except:\n",
        "                pass   \n",
        "            try:\n",
        "                OR_List.append(OR[i].get_text().strip())\n",
        "            except:\n",
        "                pass   \n",
        "            try:\n",
        "                PR_List.append(PR[i].get_text().strip())\n",
        "            except:\n",
        "                pass\n",
        "            try:\n",
        "                Team_List.append(Team[i].get_text().strip())\n",
        "            except:\n",
        "                pass   \n",
        "            try:\n",
        "                contract_List.append(contract[i].get_text().strip())\n",
        "            except:\n",
        "                pass   \n",
        "            try:\n",
        "                height_List.append(height[i].get_text().strip())\n",
        "            except:\n",
        "                pass   \n",
        "            try:\n",
        "                weight_List.append(weight[i].get_text().strip())\n",
        "            except:\n",
        "                pass   \n",
        "            try:\n",
        "                PF_List.append(PF[i].get_text().strip())\n",
        "            except:\n",
        "                pass   \n",
        "            try:\n",
        "                Value_List.append(Value[i].get_text().strip())\n",
        "            except:\n",
        "                pass\n",
        "            try:\n",
        "                FullName_List.append(FullName[i]['data-tooltip'].strip())\n",
        "            except:\n",
        "                pass\n",
        "            \n",
        "        sys.stdout.write(\"\\r{0} players have just scraped from SoFIFA!\".format(len(Players_List)))\n",
        "        sys.stdout.flush()\n",
        "        \n",
        "    df = pd.DataFrame({\"Name\":Players_List, \"Age\": Age_List, \"Overall Rating\":OR_List, \"Potential\":PR_List, \"Team\":Team_List, \"Contract expiry\":contract_List, \"Height\":height_List,\"Weight\":weight_List, \"Strong foot\":PF_List, \"Value\":Value_List, \"Full Name\":FullName_List})\n",
        "    \n",
        "    pageTree = requests.get(page+str(0), headers=headers)\n",
        "    pageSoup = BeautifulSoup(pageTree.content, 'html.parser')\n",
        "    \n",
        "    for i in pageSoup.find_all('a', {'class':\"bp3-menu-item\"}):\n",
        "        if year == i['href'][4:10]:\n",
        "            if 'FIFA' in i.get_text():\n",
        "                df['year'] = i.get_text()\n",
        "            else:\n",
        "                df['date'] = i.get_text()\n",
        "             \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2280 players have just scraped from SoFIFA!"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-810cf23b59ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_pages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mfinal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\r{0} players have just scraped from FIFA!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-318d455d341f>\u001b[0m in \u001b[0;36mextract_weights\u001b[0;34m(year, num_pages)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mPlayers_List\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mAge_List\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mOR_List\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mPR_List\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "for i in urls_list:\n",
        "    if i == urls_list[0]:\n",
        "        final_df = extract_weights(i,num_pages=40)\n",
        "        sys.stdout.write(\"\\r{0} players have just scraped from FIFA!\".format(len(final_df)))\n",
        "        sys.stdout.flush()\n",
        "    else:\n",
        "        df = extract_weights(i,num_pages=40)\n",
        "        final_df = pd.concat([final_df,df])\n",
        "        sys.stdout.write(\"\\r{0} players have just scraped from FIFA!\".format(len(final_df)))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "final_df.to_csv('player_weights.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Extract Injuries.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "455814cbe19b6b726e0d196bfa6b1ac9f2f819a90a8d5422fb186378bae9098e"
    },
    "kernelspec": {
      "display_name": "Python 3.7.3 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
